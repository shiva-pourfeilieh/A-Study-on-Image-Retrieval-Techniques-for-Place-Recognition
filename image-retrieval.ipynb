{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Install required libraries and their versions\n","\n","!pip install torch==1.13.1+cu116 \n","!pip install torchvision==0.14.1+cu116\n","!pip install faiss-cpu==1.7.3\n","!pip install pytorch-lightning==1.9.4\n","!pip install pytorch-metric-learning==2.0.1\n","!pip install opencv-python==4.7.0.72\n","!pip install scikit-image==0.19.3\n","!conda install -y gdown"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Libraries\n","\n","import os\n","import faiss\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torchvision.models\n","import pytorch_lightning as pl\n","from typing import Tuple\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","from pytorch_metric_learning import losses\n","from torchvision import transforms as tfm\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","from pytorch_lightning import loggers as pl_loggers\n","from glob import glob\n","from sklearn.neighbors import NearestNeighbors\n","from collections import defaultdict\n","\n","# Google libraries\n","\n","import requests\n","import zipfile\n","import gdown"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Download, extract and delete for google drive\n","\n","# sheewa dataset\n","\n","# zip_url_tokyo = 'https://drive.google.com/file/d/1FUSHfFUMcPyXMbzt9hKUoYoLZ_uEHULP/view?usp=drive_link' # tokyo\n","# zip_url_sf = 'https://drive.google.com/file/d/10wCmksu4w1uMRnmDkTPvuVWq_u5nqHEQ/view?usp=drive_link' # sf\n","# zip_url_gsv = 'https://drive.google.com/file/d/10QHcLRefihtVIFuLMxcnxDSi_6E4MBJs/view?usp=drive_link' # gsv\n","\n","# prof dataset\n","\n","zip_url_tokyo = 'https://drive.google.com/file/d/15QB3VNKj93027UAQWv7pzFQO1JDCdZj2/view?usp=drive_link' # tokyo\n","zip_url_sf = 'https://drive.google.com/file/d/1tQqEyt3go3vMh4fj_LZrRcahoTbzzH-y/view?usp=drive_link' # sf\n","zip_url_gsv = 'https://drive.google.com/file/d/1q7usSe9_5xV5zTfN-1In4DlmF5ReyU_A/view?usp=drive_link' # gsv\n","\n","\n","destination_folder = './dataset'\n","\n","if not os.path.exists(destination_folder):\n","    os.makedirs(destination_folder)\n","    \n","def download_and_extract(zip_url, destination_folder):\n","    # Extract the file ID from the URL\n","    file_id = zip_url.split('/')[-2]\n","    \n","    # Create the direct download URL\n","    download_url = f'https://drive.google.com/uc?id={file_id}'\n","    \n","    # Download the zip file\n","    output_file = os.path.join(destination_folder, f'{file_id}.zip')\n","    gdown.download(download_url, output_file, quiet=False, fuzzy=True)\n","    \n","    # Extract the zip file\n","    with zipfile.ZipFile(output_file, 'r') as zip_ref:\n","        zip_ref.extractall(destination_folder)\n","    \n","    # Remove the downloaded zip file\n","    os.remove(output_file)\n","    \n","    print(f\"Extraction completed for: {file_id}.zip\")\n","    \n","\n","download_and_extract(zip_url_tokyo, destination_folder)\n","download_and_extract(zip_url_sf, destination_folder)\n","download_and_extract(zip_url_gsv, destination_folder)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Transformation + Converts to RGB Format\n","\n","def open_image(path):\n","    return Image.open(path).convert(\"RGB\")\n","\n","transform = tfm.Compose([\n","    tfm.ToTensor(),\n","    tfm.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Dataset for Training\n","\n","class TrainDataset(Dataset):\n","    \"\"\"\n","    Custom dataset for training a deep learning model on images from a folder structure.\n","\n","    This class ensures each \"place\" (identified in the path) has at least a minimum number of images.\n","    It loads, preprocesses, and returns batches of images from different places.\n","    \"\"\"\n","    \n","    def __init__(self, dataset_folder, transform, img_per_place=4, min_img_per_place=4):\n","        super().__init__()\n","        self.dataset_folder = dataset_folder\n","        self.images_paths = sorted(glob(f\"{dataset_folder}/**/*.jpg\", recursive=True))\n","        self.dict_place_paths = defaultdict(list)\n","        \n","        for image_path in self.images_paths:\n","            place_id = image_path.split(\"@\")[-2]\n","            self.dict_place_paths[place_id].append(image_path)\n","\n","        assert img_per_place <= min_img_per_place, \\\n","            f\"img_per_place should be less than {min_img_per_place}\"\n","        self.img_per_place = img_per_place\n","        self.transform = transform\n","\n","        # keep only places depicted by at least min_img_per_place images\n","        for place_id in list(self.dict_place_paths.keys()):\n","            all_paths_from_place_id = self.dict_place_paths[place_id]\n","            if len(all_paths_from_place_id) < min_img_per_place:\n","                del self.dict_place_paths[place_id]\n","        self.places_ids = sorted(list(self.dict_place_paths.keys()))\n","        self.total_num_images = sum([len(paths) for paths in self.dict_place_paths.values()])\n","\n","    # extract placeId....\n","    def __getitem__(self, index):\n","        place_id = self.places_ids[index]\n","        all_paths_from_place_id = self.dict_place_paths[place_id]\n","        chosen_paths = np.random.choice(all_paths_from_place_id, self.img_per_place)\n","        images = [Image.open(path).convert('RGB') for path in chosen_paths]\n","        images = [self.transform(img) for img in images]\n","        return torch.stack(images), torch.tensor(index).repeat(self.img_per_place)\n","\n","    def __len__(self):\n","        return len(self.places_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# DataLoader\n","\n","train_dataset = TrainDataset('/kaggle/working/dataset/gsv_xs/train', transform)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=64, num_workers=2, shuffle=True)\n","print(f\"Train dataset: {len(train_dataset)}\")\n","# print(train_dataset.total_num_images)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Dataset for Testing\n","\n","class TestDataset(Dataset):\n","    def __init__(self, dataset_folder, database_folder=\"database\", queries_folder=\"queries\", positive_dist_threshold=25):\n","        super().__init__()\n","        self.dataset_folder = dataset_folder\n","        self.database_folder = os.path.join(dataset_folder, database_folder)\n","        self.queries_folder = os.path.join(dataset_folder, queries_folder)\n","        self.dataset_name = os.path.basename(dataset_folder)\n","        self.database_paths = sorted(glob(os.path.join(self.database_folder, \"**\", \"*.jpg\"), recursive=True))\n","        self.queries_paths = sorted(glob(os.path.join(self.queries_folder, \"**\", \"*.jpg\"),  recursive=True))\n","\n","        # exteract UTM\n","        self.database_utms = np.array \\\n","            ([(path.split(\"@\")[1], path.split(\"@\")[2]) for path in self.database_paths]).astype(float)\n","        self.queries_utms = np.array([(path.split(\"@\")[1], path.split(\"@\")[2]) for path in self.queries_paths]).astype \\\n","            (float)\n","\n","        # Find positives_per_query, which are within positive_dist_threshold\n","        knn = NearestNeighbors(n_jobs=-1)\n","        knn.fit(self.database_utms)\n","        self.positives_per_query = knn.radius_neighbors(self.queries_utms, radius=positive_dist_threshold, return_distance=False)\n","\n","        self.images_paths = [p for p in self.database_paths]\n","        self.images_paths += [p for p in self.queries_paths]\n","\n","        self.database_num = len(self.database_paths)\n","        self.queries_num = len(self.queries_paths)\n","\n","    def __getitem__(self, index):\n","        image_path = self.images_paths[index]\n","        pil_img = open_image(image_path)\n","        normalized_img = transform(pil_img)\n","        return normalized_img, index\n","\n","    def __len__(self):\n","        return len(self.images_paths)\n","\n","    def __repr__(self):\n","        return f\"< {self.dataset_name} - #q: {self.queries_num}; #db: {self.database_num} >\"\n","\n","    def get_positives(self):\n","        return self.positives_per_query"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# DataLoader Test SF and val SF + Test Tokyo\n","\n","sf_val_dataset = TestDataset('/kaggle/working/dataset/sf_xs/val')\n","sf_test_dataset = TestDataset('/kaggle/working/dataset/sf_xs/test')\n","tokyo_test_dataset = TestDataset('/kaggle/working/dataset/tokyo_xs/test')\n","\n","val_loader = DataLoader(dataset=sf_val_dataset, batch_size=64, num_workers=2, shuffle=False)\n","test_loader = DataLoader(dataset=sf_test_dataset, batch_size=64, num_workers=2, shuffle=False)\n","tokyo_loader = DataLoader(dataset=tokyo_test_dataset, batch_size=64, num_workers=2, shuffle=False)\n","\n","print(f\"val sf: {len(sf_val_dataset)}\")\n","print(f\"test sf: {len(sf_test_dataset)}\")\n","print(f\"test tokyo: {len(tokyo_test_dataset)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# GeM layer\n","\n","class GeM(nn.Module):\n","    def __init__(self, p=3, eps=1e-6):\n","        super().__init__()\n","        self.p = nn.Parameter(torch.ones(1) * p)\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        return self.gem(x, p=self.p, eps=self.eps)\n","\n","    def gem(self, x, p=3, eps=1e-6):\n","        return nn.functional.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1.0 / p)\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + f\"(p={self.p.data.tolist()[0]:.4f}, eps={self.eps})\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Compute Recall\n","\n","def compute_recalls(eval_ds: Dataset, queries_descriptors : np.ndarray, database_descriptors : np.ndarray):\n","\n","    # Use a kNN to find predictions\n","    faiss_index = faiss.IndexFlatL2(queries_descriptors.shape[1])\n","    faiss_index.add(database_descriptors)\n","    del database_descriptors\n","\n","    print(\"Calculating\")\n","    RECALL_VALUES = [1, 5]\n","    _, predictions = faiss_index.search(queries_descriptors, max(RECALL_VALUES))\n","\n","    positives_per_query = eval_ds.get_positives()\n","    recalls = np.zeros(len(RECALL_VALUES))\n","    for query_index, preds in enumerate(predictions):\n","        for i, n in enumerate(RECALL_VALUES):\n","            if np.any(np.in1d(preds[:n], positives_per_query[query_index])):\n","                recalls[i:] += 1\n","                break\n","    # Divide by queries_num and multiply by 100, so the recalls are in percentages\n","    recalls = recalls / eval_ds.queries_num * 100\n","    recalls_str = \", \".join([f\"R@{val}: {rec:.1f}\" for val, rec in zip(RECALL_VALUES, recalls)])\n","    \n","    return recalls, recalls_str"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Step 1 - AVG pooling with Adam with ContrastiveLoss\n","\n","class LightningModel(pl.LightningModule):\n","    def __init__(self, val_dataset, test_dataset, descriptors_dim=512):\n","        super().__init__()\n","        self.val_dataset = val_dataset\n","        self.test_dataset = test_dataset\n","        \n","        # Load pretrained ResNet-18\n","        resnet18 = torchvision.models.resnet18(pretrained=True)\n","        \n","        # Truncate the model at conv3 (end of layer2)\n","        self.features = torch.nn.Sequential(\n","            resnet18.conv1,\n","            resnet18.bn1,\n","            resnet18.relu,\n","            resnet18.maxpool,\n","            resnet18.layer1,\n","            resnet18.layer2,\n","        )\n","        \n","        # Add average pooling layer\n","        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n","        \n","        # Calculate the output dimension\n","        with torch.no_grad():\n","            dummy_input = torch.randn(1, 3, 224, 224)\n","            output = self.avgpool(self.features(dummy_input))\n","            self.output_dim = output.view(-1).shape[0]\n","        \n","        # Add a linear layer to match the desired descriptors dimension\n","        self.fc = torch.nn.Linear(self.output_dim, descriptors_dim)\n","        \n","        # Set the loss function\n","        self.loss_fn = losses.ContrastiveLoss(pos_margin=0, neg_margin=1)\n","\n","    def forward(self, images):\n","        x = self.features(images)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        descriptors = self.fc(x)\n","        return descriptors\n","        \n","        \n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n","        return optimizer\n","\n","    def loss_function(self, descriptors, labels):\n","        loss = self.loss_fn(descriptors, labels)\n","        return loss\n","\n","    def training_step(self, batch, batch_idx):\n","        images, labels = batch\n","        num_places, num_images_per_place, C, H, W = images.shape\n","        images = images.view(num_places * num_images_per_place, C, H, W)\n","        labels = labels.view(num_places * num_images_per_place)\n","\n","        descriptors = self(images)\n","        loss = self.loss_function(descriptors, labels)\n","        \n","        self.log('loss', loss.item(), logger=True)\n","        return {'loss': loss}\n","\n","    def inference_step(self, batch):\n","        images, _ = batch\n","        descriptors = self(images)\n","        return descriptors.cpu().numpy().astype(np.float32)\n","\n","    def validation_step(self, batch, batch_idx):\n","        return self.inference_step(batch)\n","\n","    def test_step(self, batch, batch_idx):\n","        return self.inference_step(batch)\n","\n","    def validation_epoch_end(self, all_descriptors):\n","        return self.inference_epoch_end(all_descriptors, self.val_dataset, 'val')\n","\n","    def test_epoch_end(self, all_descriptors):\n","        return self.inference_epoch_end(all_descriptors, self.test_dataset, 'test')\n","\n","    def inference_epoch_end(self, all_descriptors, inference_dataset, split):\n","        all_descriptors = np.concatenate(all_descriptors)\n","        queries_descriptors = all_descriptors[inference_dataset.database_num:]\n","        database_descriptors = all_descriptors[:inference_dataset.database_num]\n","\n","        recalls, recalls_str = compute_recalls(inference_dataset, queries_descriptors, database_descriptors)\n","\n","        print(f\"Epoch[{self.current_epoch:02d}]): \" +\n","                      f\"recalls: {recalls_str}\")\n","\n","        self.log(f'{split}/R@1', recalls[0], prog_bar=False, logger=True)\n","        self.log(f'{split}/R@5', recalls[1], prog_bar=False, logger=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Step 2 - GeM with Adam with ContrastiveLoss\n","\n","class LightningModel(pl.LightningModule):\n","    def __init__(self, val_dataset, test_dataset, descriptors_dim=512):\n","        super().__init__()\n","        self.val_dataset = val_dataset\n","        self.test_dataset = test_dataset\n","        \n","        # Load pretrained ResNet-18\n","        resnet18 = torchvision.models.resnet18(pretrained=True)\n","        \n","        # Truncate the model at conv3 (end of layer2)\n","        self.features = torch.nn.Sequential(\n","            resnet18.conv1,\n","            resnet18.bn1,\n","            resnet18.relu,\n","            resnet18.maxpool,\n","            resnet18.layer1,\n","            resnet18.layer2,\n","        )\n","        \n","        # Replace average pooling with GeM pooling\n","        self.gem_pool = GeM()\n","    \n","        # Calculate the output dimension\n","        with torch.no_grad():\n","            dummy_input = torch.randn(1, 3, 224, 224)\n","            output = self.gem_pool(self.features(dummy_input))\n","            self.output_dim = output.view(-1).shape[0]\n","    \n","        # Add a linear layer to match the desired descriptors dimension\n","        self.fc = torch.nn.Linear(self.output_dim, descriptors_dim)\n","    \n","        # Set the loss function\n","        self.loss_fn = losses.ContrastiveLoss(pos_margin=0, neg_margin=1)\n","\n","    def forward(self, images):\n","        x = self.features(images)\n","        x = self.gem_pool(x)\n","        x = torch.flatten(x, 1)\n","        descriptors = self.fc(x)\n","        return descriptors\n","        \n","\n","        \n","        \n","        \n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n","        return optimizer\n","\n","    def loss_function(self, descriptors, labels):\n","        loss = self.loss_fn(descriptors, labels)\n","        return loss\n","\n","    def training_step(self, batch, batch_idx):\n","        images, labels = batch\n","        num_places, num_images_per_place, C, H, W = images.shape\n","        images = images.view(num_places * num_images_per_place, C, H, W)\n","        labels = labels.view(num_places * num_images_per_place)\n","\n","        descriptors = self(images)\n","        loss = self.loss_function(descriptors, labels)\n","        \n","        self.log('loss', loss.item(), logger=True)\n","        return {'loss': loss}\n","\n","    def inference_step(self, batch):\n","        images, _ = batch\n","        descriptors = self(images)\n","        return descriptors.cpu().numpy().astype(np.float32)\n","\n","    def validation_step(self, batch, batch_idx):\n","        return self.inference_step(batch)\n","\n","    def test_step(self, batch, batch_idx):\n","        return self.inference_step(batch)\n","\n","    def validation_epoch_end(self, all_descriptors):\n","        return self.inference_epoch_end(all_descriptors, self.val_dataset, 'val')\n","\n","    def test_epoch_end(self, all_descriptors):\n","        return self.inference_epoch_end(all_descriptors, self.test_dataset, 'test')\n","\n","    def inference_epoch_end(self, all_descriptors, inference_dataset, split):\n","        all_descriptors = np.concatenate(all_descriptors)\n","        queries_descriptors = all_descriptors[inference_dataset.database_num:]\n","        database_descriptors = all_descriptors[:inference_dataset.database_num]\n","\n","        recalls, recalls_str = compute_recalls(inference_dataset, queries_descriptors, database_descriptors)\n","\n","        print(f\"Epoch[{self.current_epoch:02d}]): \" +\n","                      f\"recalls: {recalls_str}\")\n","\n","        self.log(f'{split}/R@1', recalls[0], prog_bar=False, logger=True)\n","        self.log(f'{split}/R@5', recalls[1], prog_bar=False, logger=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# step 3 - GeM with Adam with TripletLoss\n","\n","import torch.nn.functional as F\n","\n","class TripletLoss(nn.Module):\n","    def __init__(self, margin=1.0):\n","        super(TripletLoss, self).__init__()\n","        self.margin = margin\n","\n","    def forward(self, anchor, positive, negative):\n","        distance_positive = (anchor - positive).pow(2).sum(1)\n","        distance_negative = (anchor - negative).pow(2).sum(1)\n","        losses = F.relu(distance_positive - distance_negative + self.margin)\n","        return losses.mean()\n","\n","class LightningModel(pl.LightningModule):\n","    def __init__(self, val_dataset, test_dataset, descriptors_dim=512):\n","        super().__init__()\n","        self.val_dataset = val_dataset\n","        self.test_dataset = test_dataset\n","        \n","        # Load pretrained ResNet-18\n","        resnet18 = torchvision.models.resnet18(pretrained=True)\n","        \n","        # Truncate the model at conv3 (end of layer2)\n","        self.features = torch.nn.Sequential(\n","            resnet18.conv1,\n","            resnet18.bn1,\n","            resnet18.relu,\n","            resnet18.maxpool,\n","            resnet18.layer1,\n","            resnet18.layer2,\n","        )\n","        \n","        # Replace average pooling with GeM pooling\n","        self.gem_pool = GeM()\n","    \n","        # Calculate the output dimension\n","        with torch.no_grad():\n","            dummy_input = torch.randn(1, 3, 224, 224)\n","            output = self.gem_pool(self.features(dummy_input))\n","            self.output_dim = output.view(-1).shape[0]\n","    \n","        # Add a linear layer to match the desired descriptors dimension\n","        self.fc = torch.nn.Linear(self.output_dim, descriptors_dim)\n","    \n","        # Set the loss function\n","        self.loss_fn = TripletLoss(margin=1.0)\n","\n","    def forward(self, images):\n","        x = self.features(images)\n","        x = self.gem_pool(x)\n","        x = torch.flatten(x, 1)\n","        descriptors = self.fc(x)\n","        return descriptors\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n","        return optimizer\n","\n","    def loss_function(self, descriptors, labels):\n","        # Reshape descriptors and labels\n","        num_places, num_images_per_place = labels.shape\n","        descriptors = descriptors.view(num_places, num_images_per_place, -1)\n","        \n","        # For each place, select an anchor, a positive, and a negative\n","        anchors = descriptors[:, 0]  # First image of each place as anchor\n","        positives = descriptors[:, 1]  # Second image of each place as positive\n","        \n","        # Randomly select negatives from other places\n","        negatives_idx = torch.randint(0, num_places, (num_places,))\n","        negatives = descriptors[negatives_idx, 0]\n","        \n","        loss = self.loss_fn(anchors, positives, negatives)\n","        return loss\n","\n","    def training_step(self, batch, batch_idx):\n","        images, labels = batch\n","        num_places, num_images_per_place, C, H, W = images.shape\n","        images = images.view(num_places * num_images_per_place, C, H, W)\n","\n","        descriptors = self(images)\n","        loss = self.loss_function(descriptors, labels)\n","        \n","        self.log('loss', loss.item(), logger=True)\n","        return {'loss': loss}\n","\n","\n","    def inference_step(self, batch):\n","        images, _ = batch\n","        descriptors = self(images)\n","        return descriptors.cpu().numpy().astype(np.float32)\n","\n","    def validation_step(self, batch, batch_idx):\n","        return self.inference_step(batch)\n","\n","    def test_step(self, batch, batch_idx):\n","        return self.inference_step(batch)\n","\n","    def validation_epoch_end(self, all_descriptors):\n","        return self.inference_epoch_end(all_descriptors, self.val_dataset, 'val')\n","\n","    def test_epoch_end(self, all_descriptors):\n","        return self.inference_epoch_end(all_descriptors, self.test_dataset, 'test')\n","\n","    def inference_epoch_end(self, all_descriptors, inference_dataset, split):\n","        all_descriptors = np.concatenate(all_descriptors)\n","        queries_descriptors = all_descriptors[inference_dataset.database_num:]\n","        database_descriptors = all_descriptors[:inference_dataset.database_num]\n","\n","        recalls, recalls_str = compute_recalls(inference_dataset, queries_descriptors, database_descriptors)\n","\n","        print(f\"Epoch[{self.current_epoch:02d}]): \" +\n","                      f\"recalls: {recalls_str}\")\n","\n","        self.log(f'{split}/R@1', recalls[0], prog_bar=False, logger=True)\n","        self.log(f'{split}/R@5', recalls[1], prog_bar=False, logger=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 3 - GeM with Adam with MultiSimilarityLoss\n","\n","from pytorch_metric_learning import losses\n","\n","class LightningModel(pl.LightningModule):\n","    def __init__(self, val_dataset, test_dataset, descriptors_dim=512):\n","        super().__init__()\n","        self.val_dataset = val_dataset\n","        self.test_dataset = test_dataset\n","        \n","        # Load pretrained ResNet-18\n","        resnet18 = torchvision.models.resnet18(pretrained=True)\n","        \n","        # Truncate the model at conv3 (end of layer2)\n","        self.features = torch.nn.Sequential(\n","            resnet18.conv1,\n","            resnet18.bn1,\n","            resnet18.relu,\n","            resnet18.maxpool,\n","            resnet18.layer1,\n","            resnet18.layer2,\n","        )\n","        \n","        # Replace average pooling with GeM pooling\n","        self.gem_pool = GeM()\n","    \n","        # Calculate the output dimension\n","        with torch.no_grad():\n","            dummy_input = torch.randn(1, 3, 224, 224)\n","            output = self.gem_pool(self.features(dummy_input))\n","            self.output_dim = output.view(-1).shape[0]\n","    \n","        # Add a linear layer to match the desired descriptors dimension\n","        self.fc = torch.nn.Linear(self.output_dim, descriptors_dim)\n","    \n","        # Set the loss function to MultiSimilarityLoss\n","        self.loss_fn = losses.MultiSimilarityLoss()\n","\n","    def forward(self, images):\n","        x = self.features(images)\n","        x = self.gem_pool(x)\n","        x = torch.flatten(x, 1)\n","        descriptors = self.fc(x)\n","        return descriptors\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n","        return optimizer\n","\n","    def loss_function(self, descriptors, labels):\n","        loss = self.loss_fn(descriptors, labels)\n","        return loss\n","\n","    def training_step(self, batch, batch_idx):\n","        images, labels = batch\n","        num_places, num_images_per_place, C, H, W = images.shape\n","        images = images.view(num_places * num_images_per_place, C, H, W)\n","        labels = labels.view(num_places * num_images_per_place)\n","\n","        descriptors = self(images)\n","        loss = self.loss_function(descriptors, labels)\n","        \n","        self.log('loss', loss.item(), logger=True)\n","        return {'loss': loss}\n","\n","\n","    def inference_step(self, batch):\n","        images, _ = batch\n","        descriptors = self(images)\n","        return descriptors.cpu().numpy().astype(np.float32)\n","\n","    def validation_step(self, batch, batch_idx):\n","        return self.inference_step(batch)\n","\n","    def test_step(self, batch, batch_idx):\n","        return self.inference_step(batch)\n","\n","    def validation_epoch_end(self, all_descriptors):\n","        return self.inference_epoch_end(all_descriptors, self.val_dataset, 'val')\n","\n","    def test_epoch_end(self, all_descriptors):\n","        return self.inference_epoch_end(all_descriptors, self.test_dataset, 'test')\n","\n","    def inference_epoch_end(self, all_descriptors, inference_dataset, split):\n","        all_descriptors = np.concatenate(all_descriptors)\n","        queries_descriptors = all_descriptors[inference_dataset.database_num:]\n","        database_descriptors = all_descriptors[:inference_dataset.database_num]\n","\n","        recalls, recalls_str = compute_recalls(inference_dataset, queries_descriptors, database_descriptors)\n","\n","        print(f\"Epoch[{self.current_epoch:02d}]): \" +\n","                      f\"recalls: {recalls_str}\")\n","\n","        self.log(f'{split}/R@1', recalls[0], prog_bar=False, logger=True)\n","        self.log(f'{split}/R@5', recalls[1], prog_bar=False, logger=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Step 4 - GeM with MultiSimilarityLoss with ADAMW + SGD\n","\n","from pytorch_metric_learning import losses\n","\n","class LightningModel(pl.LightningModule):\n","    def __init__(self, val_dataset, test_dataset, descriptors_dim=512):\n","        super().__init__()\n","        self.val_dataset = val_dataset\n","        self.test_dataset = test_dataset\n","        \n","        # Load pretrained ResNet-18\n","        resnet18 = torchvision.models.resnet18(pretrained=True)\n","        \n","        # Truncate the model at conv3 (end of layer2)\n","        self.features = torch.nn.Sequential(\n","            resnet18.conv1,\n","            resnet18.bn1,\n","            resnet18.relu,\n","            resnet18.maxpool,\n","            resnet18.layer1,\n","            resnet18.layer2,\n","        )\n","        \n","        # Replace average pooling with GeM pooling\n","        self.gem_pool = GeM()\n","    \n","        # Calculate the output dimension\n","        with torch.no_grad():\n","            dummy_input = torch.randn(1, 3, 224, 224)\n","            output = self.gem_pool(self.features(dummy_input))\n","            self.output_dim = output.view(-1).shape[0]\n","    \n","        # Add a linear layer to match the desired descriptors dimension\n","        self.fc = torch.nn.Linear(self.output_dim, descriptors_dim)\n","    \n","        # Set the loss function to MultiSimilarityLoss\n","        self.loss_fn = losses.MultiSimilarityLoss()\n","\n","    def forward(self, images):\n","        x = self.features(images)\n","        x = self.gem_pool(x)\n","        x = torch.flatten(x, 1)\n","        descriptors = self.fc(x)\n","        return descriptors\n","\n","    \n","    \n","#     def configure_optimizers(self):\n","#         optimizer = torch.optim.AdamW(self.parameters(), lr=1e-5, weight_decay=0.01)\n","#         return optimizer\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.SGD(self.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n","        return optimizer\n","\n","    def loss_function(self, descriptors, labels):\n","        loss = self.loss_fn(descriptors, labels)\n","        return loss\n","\n","    def training_step(self, batch, batch_idx):\n","        images, labels = batch\n","        num_places, num_images_per_place, C, H, W = images.shape\n","        images = images.view(num_places * num_images_per_place, C, H, W)\n","        labels = labels.view(num_places * num_images_per_place)\n","\n","        descriptors = self(images)\n","        loss = self.loss_function(descriptors, labels)\n","        \n","        self.log('loss', loss.item(), logger=True)\n","        return {'loss': loss}\n","\n","\n","    def inference_step(self, batch):\n","        images, _ = batch\n","        descriptors = self(images)\n","        return descriptors.cpu().numpy().astype(np.float32)\n","\n","    def validation_step(self, batch, batch_idx):\n","        return self.inference_step(batch)\n","\n","    def test_step(self, batch, batch_idx):\n","        return self.inference_step(batch)\n","\n","    def validation_epoch_end(self, all_descriptors):\n","        return self.inference_epoch_end(all_descriptors, self.val_dataset, 'val')\n","\n","    def test_epoch_end(self, all_descriptors):\n","        return self.inference_epoch_end(all_descriptors, self.test_dataset, 'test')\n","\n","    def inference_epoch_end(self, all_descriptors, inference_dataset, split):\n","        all_descriptors = np.concatenate(all_descriptors)\n","        queries_descriptors = all_descriptors[inference_dataset.database_num:]\n","        database_descriptors = all_descriptors[:inference_dataset.database_num]\n","\n","        recalls, recalls_str = compute_recalls(inference_dataset, queries_descriptors, database_descriptors)\n","\n","        print(f\"Epoch[{self.current_epoch:02d}]): \" +\n","                      f\"recalls: {recalls_str}\")\n","\n","        self.log(f'{split}/R@1', recalls[0], prog_bar=False, logger=True)\n","        self.log(f'{split}/R@5', recalls[1], prog_bar=False, logger=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = LightningModel(sf_val_dataset, sf_test_dataset)\n","\n","checkpoint_cb = ModelCheckpoint(\n","    monitor='val/R@1',\n","    filename='_epoch({epoch:02d})_R@1[{val/R@1:.4f}]_R@5[{val/R@5:.4f}]',\n","    auto_insert_metric_name=False,\n","    save_weights_only=False,\n","    save_top_k=1,\n","    save_last=True,\n","    mode='max'\n",")\n","\n","tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"logs/\", version=\"default\")\n","\n","trainer = pl.Trainer(\n","    accelerator='gpu',\n","    devices=[0],\n","    default_root_dir='./logs',\n","    num_sanity_val_steps=0,\n","    precision=16,\n","    max_epochs=10,\n","    check_val_every_n_epoch=1,\n","    logger=tb_logger,\n","    callbacks=[checkpoint_cb],\n","    reload_dataloaders_every_n_epochs=1,\n","    log_every_n_steps=20,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"Traning Process\\n\")\n","trainer.fit(model=model, ckpt_path=None, train_dataloaders=train_loader, val_dataloaders=val_loader)\n","print(\"VALIDATING ON SF\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"-_-_-_-_-_-_-_-_-_TESTING ON SF-_-_-_-_-_-_-_-_-_-_-_\\n\")\n","trainer.test(model=model, dataloaders=test_loader, ckpt_path='best')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = LightningModel(sf_val_dataset, tokyo_test_dataset)\n","\n","print(\"-_-_-_-_-_-_-_-_-_TESTING ON TOKYO-_-_-_-_-_-_-_-_-_-_-_\")\n","trainer.test(model=model, dataloaders=tokyo_loader, ckpt_path= None)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
